import streamlit as st
from openai import OpenAI

from pydantic import BaseModel

OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
client = OpenAI(api_key=OPENAI_API_KEY)

st.title("Contamination and Unfairness Checker üéì")
st.write("Submit your base code and user prompt!")

base_code = st.text_area("Base Code", "Provide the base code")
user_prompt = st.text_area("Prompt", "Provide the prompt.")


class TaskData(BaseModel):
    judge_response: str


class LLMJudge(TaskData):
    contaminated: bool
    contamination_link: str
    ambiguous_unfair: bool
    problem_description: str
    suggested_fix: str


def get_review(prompt, system_message, response_format):
    response = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": (
                    system_message
                )
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        response_format=response_format,
    )

    return response.choices[0].message.parsed


if st.button("Evaluate"):
    with st.spinner("Evaluating..."):
        system_prompt = """
        Here‚Äôs a system prompt tailored for detecting contamination and unfairness in code-related prompts ‚Äî particularly helpful when evaluating coding tasks like the one you‚Äôve shared:

‚∏ª

You are an expert coding task evaluator. Your job is to detect and report whether a programming prompt suffers from:
	1.	Contamination ‚Äî The task or its solution can be found online in the same or a very similar form (e.g., LeetCode, Stack Overflow, GeeksforGeeks).
	2.	Unfairness ‚Äî The task‚Äôs requirements are vague, inconsistent, or imply behavior that is under-specified (e.g., requiring time extraction logic without defining what constitutes a valid time).

Your output must follow this schema and provide clarity for educators or reviewers designing new problems.

‚úÖ Structure of Your Output (for JSON parsing):

{
  "contaminated": <true|false>,
  "contamination_link": "<if contaminated, give URL to matching problem or leave blank>",
  "ambiguous_unfair": <true|false>,
  "problem_description": "<why it is contaminated or unfair (e.g., too similar to GeeksforGeeks post or unclear rules)>",
  "suggested_fix": "<how to make the task more unique or fair>"
}

‚úÖ Guidelines to Determine Contamination:
	‚Ä¢	Search for core logic or task phrasing online.
	‚Ä¢	If similar code or the same objective exists with minor variations, flag it as contaminated.

‚úÖ Guidelines to Determine Unfairness:
	‚Ä¢	Look for implied behavior not explicitly defined (e.g., multiple valid interpretations of the goal).
	‚Ä¢	Check whether required outputs (e.g., overlapping time extraction) are fully explained.

‚∏ª

‚úÖ Examples of Suggested Fixes for Contaminated or Unfair Prompts:
	‚Ä¢	To decontaminate: Introduce additional constraints, dynamic inputs, or new output formats. For example:
	‚Ä¢	‚ÄúOnly return the earliest valid time found.‚Äù
	‚Ä¢	‚ÄúSupport delimiters other than colons (e.g., |, ,).‚Äù
	‚Ä¢	To remove unfairness: Define time validity clearly (e.g., must match hh:mm:ss with 0 <= hh < 24, etc.), and clarify how overlapping cases should be handled.

‚∏ª

Let me know if you‚Äôd like this as a reusable function, or integrated into a tool for reviewing batches of prompts.
        """
        response_format = LLMJudge

        prompt = str({
            'base_code': base_code,
            "prompt": user_prompt
        })

        response = get_review(prompt, system_prompt, response_format)

        result = response
        st.subheader("Evaluation Result:")
        st.write(result)